{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS is a capstone project from--Microsoft Professional Program in Artificial Intelligence | edX course (DAT264x)\n",
    "#### natural language processing, multi-class, multi-label classification challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call modules\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import RidgeClassifier,PassiveAggressiveClassifier,SGDClassifier,LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call modules\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score,precision_score, confusion_matrix,jaccard_similarity_score,roc_auc_score,f1_score,hamming_loss,make_scorer\n",
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import re\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV,train_test_split\n",
    "from sklearn import naive_bayes ,metrics\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif,SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlp_functions as nlp\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load origin data from csv file\n",
    "train_values=pd.read_csv('/home/ajshemi/DAT264x_Identifying_Topics_of_World_Bank_Publications/train_values.csv',index_col=0,header=0)\n",
    "test_values=pd.read_csv('/home/ajshemi/DAT264x_Identifying_Topics_of_World_Bank_Publications/test_values.csv',index_col=0,header=0)\n",
    "train_labels=pd.read_csv('/home/ajshemi/DAT264x_Identifying_Topics_of_World_Bank_Publications/train_labels.csv',index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up train data and save data (not shown here)\n",
    "#retrieve data, load data\n",
    "X=pd.read_csv('X_data_1.csv',header=0,dtype=np.object_).iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling np.nan\n",
    "X_nan=pd.Series(np.where(X.isna()==True,X.values.astype('U'),X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_nan\n",
    "#X_Set=X_Set_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_features=20000,stop_words='english',ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training dataset and testing dataset\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X.values, train_labels.values,test_size=0.25,random_state=0)\n",
    "#X_train_raw, X_test_raw, y_train, y_test = train_test_split(pd.read_csv('X_data_1.csv',header=0,dtype= np.object_).iloc[:,-1].values, train_labels.values,test_size=0.0,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_transform training data\n",
    "#X_train=vectorizer.fit_transform(X_train_raw).toarray()\n",
    "X_train=vec.fit_transform(X_train_raw)#toarray() #?list object has no attribute lower #using join removes the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14122, 20000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape #, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=vec.transform(X_test_raw)#.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "#df_pred=pd.DataFrame()\n",
    "col=[]\n",
    "for i in range(y_train.shape[1]):\n",
    "    #select best features\n",
    "    select=SelectKBest(chi2, k=7500)\n",
    "    #select=SelectKBest(f_classif, k=7500)\n",
    "    #select=SelectPercentile(f_classif,percentile=10)\n",
    "    \n",
    "    #fit and transform data using best features\n",
    "    X_train_sel=select.fit_transform(X_train,y_train[:,i]).toarray()\n",
    "    X_test_sel=select.transform(X_test).toarray()\n",
    "    \n",
    "    #called model and fit model\n",
    "    base_SVC=LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "    ovr_SVC=OneVsRestClassifier(base_SVC)\n",
    "    ovr_SVC.fit(X_train_sel, y_train[:,i])\n",
    "    \n",
    "    #predict model\n",
    "    col = 'y_pred_%d' % i\n",
    "    df[col] = ovr_SVC.predict(X_test_sel)\n",
    "    \n",
    "    #X_Test_sel=select.transform(X_Test).toarray()\n",
    "    #ovr_RC.predict(X_test_sel)\n",
    "    #df_pred[col]=ovr_SVC.predict(X_Test_sel)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test, df.values,average='micro'),metrics.accuracy_score(y_test, df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Table of selected results\n",
    "(0.6196887974855458, 0.22897196261682243)(1,2)SelectKBest(chi2, k=7500),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6150645347717202, 0.22323704333050126)(1,1)SelectKBest(chi2, k=10000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6070543824466202, 0.22323704333050126)(2,2)SelectKBest(chi2, k=10000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6246828143021915, 0.2336448598130841)(1,2)SelectKBest(chi2, k=10000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6157903022670025, 0.225785896346644)(1,3)SelectKBest(chi2, k=5000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.5903346925592474, 0.2143160577740017) (2,3)SelectKBest(chi2, k=5000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6247308520455245, 0.23237043330501275)(1,2)SelectKBest(chi2, k=10000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6178346441288101, 0.22536108751062023)(1,2)SelectKBest(chi2, k=5000),LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.5809917355371901, 0.12213254035683942) selectPercentile=10 LinearSVC(tol=1e-3,max_iter=1000,class_weight='balanced')\n",
    "(0.5846345385692794, 0.12383177570093458) SelectKBest(chi2, k=2028) LinearSVC(tol=1e-3,max_iter=1000,class_weight='balanced')\n",
    "(0.6033900996387599, 0.21771452846219203) SelectKBest(chi2, k=2028) LinearSVC(tol=1e-3,max_iter=1000,class_weight=None)\n",
    "(0.6050980392156863, 0.21771452846219203) SelectKBest(chi2, k=2028) SGDClassifier(loss='modified_huber',tol=1e-3)\n",
    "(0.5617931148353009, 0.19923534409515717) SelectKBest(chi2, k=2028) GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### principal component analysis approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_pca=TruncatedSVD(n_components=5000)\n",
    "alt_pca.fit(X_train)\n",
    "X_train_alt_pca=alt_pca.transform(X_train)\n",
    "X_test_alt_pca=alt_pca.transform(X_test)\n",
    "X_test_alt_pca.shape,X_train_alt_pca.shape,type(X_test_alt_pca),type(X_train_alt_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca=PCA(n_components=2500)\n",
    "#pca.fit(X_train)\n",
    "#X_train_pca=pca.transform(X_train)\n",
    "#X_test_pca=pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_Test_alt_pca=alt_pca.transform(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter testing. RidgeClassifier. alternate pca\n",
    "f1score=[]\n",
    "accuscore=[]\n",
    "#max_iterations=[1000,5000]\n",
    "alpha=[0.001,0.0001]\n",
    "tol=[1e-3,1e-6]\n",
    "#class_weights=[None,'balanced']\n",
    "for i in alpha:\n",
    "    for j in tol:\n",
    "        #base_SVC=LinearSVC(tol=1e-3,max_iter=i,class_weight=j)\n",
    "        #ovr_SVC=OneVsRestClassifier(base_SVC)\n",
    "        #ovr_SVC.fit(X_train_alt_pca, y_train)\n",
    "        \n",
    "        base_RC=RidgeClassifier(tol=j,class_weight=None,alpha=i)\n",
    "        ovr_RC=OneVsRestClassifier(base_RC)\n",
    "        ovr_RC.fit(X_train_alt_pca, y_train)\n",
    "        f1score.append(metrics.f1_score(y_test, ovr_RC.predict(X_test_alt_pca),average='micro'))\n",
    "        accuscore.append(metrics.accuracy_score(y_test, ovr_RC.predict(X_test_alt_pca)))\n",
    "\n",
    "print(f1score)\n",
    "print(accuscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol=1e-3\n",
    "0.6182317701377494, 0.22026338147833474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics.f1_score(y_test, ovr_RC.predict(X_test),average='micro'),metrics.accuracy_score(y_test, ovr_RC.predict(X_test))\n",
    "metrics.f1_score(y_test, ovr_RC.predict(X_test_alt_pca),average='micro'),metrics.accuracy_score(y_test, ovr_RC.predict(X_test_alt_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard_similarity_score(y_test, ovr_RC.predict(X_test)),hamming_loss(y_test, ovr_RC.predict(X_test))\n",
    "jaccard_similarity_score(y_test, ovr_RC.predict(X_test_alt_pca)),hamming_loss(y_test, ovr_RC.predict(X_test_alt_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up and prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given test data; test_values\n",
    "X_Test=test_values['doc_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation; clean test data\n",
    "X_Test=X_Test.apply(lambda x: nlp.clean_up_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split_words function\n",
    "X_Test=X_Test.apply(lambda x: split_words(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use splitW function\n",
    "X_Test=X_Test.apply(lambda x: splitW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize test data\n",
    "X_Test=X_Test.apply(lambda x: nlp.tokenize(x, min_char=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize test data\n",
    "X_Test=X_Test.apply(lambda x: [lm.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create string of tokenized text\n",
    "X_Test=X_Test.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem text\n",
    "X_Test=X_Test.apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv file\n",
    "pd.DataFrame(X_Test).to_csv('X_Test_data_1.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve data, load data\n",
    "X_Test=pd.read_csv('X_Test_data_1.csv',header=0,dtype= np.object_).iloc[:,-1]#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling np.nan\n",
    "X_Test_nan=pd.Series(np.where(X_Test.isna()==True,X_Test.values.astype('U'),X_Test.values))\n",
    "X_Test=X_Test_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform test data\n",
    "#X_Test=vectorizer.transform(X_Test).toarray()\n",
    "X_Test=vectorizer.transform(pd.read_csv('X_Test_data_1.csv',header=0,dtype= np.object_ ).iloc[:,-1].values)#.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
